# Софийски университет „Св. Кл. Охридски”
# Факултет по математика и информатика

## Курсов проект (шаблон Course Project 2) — попълнена документация

**Тема:** Domain-Specific Intelligent Product Search Engine (*семантично търсене в продуктови каталози*)  
**Репозитори:** `fmiunisofia-semantic-search`  
**Студент:** _Име Презиме Фамилия_  
**Ф.Н.:** _XXXXXXXX_  
**Курс/дисциплина:** _XXXXXXXX_  
**Учебна година:** _2025/2026_  
**Преподавател(и):** _XXXXXXXX_  
**Консултант(и):** _ако има_  

---

## Декларация за липса на плагиатство

Плагиатство е да използваш идеи, мнение или работа на друг, като претендираш, че са твои. Това е форма на преписване. Тази курсова работа е моя, като всички изречения, илюстрации и програми от други хора са изрично цитирани. Тази курсова работа или нейна версия не са представени в друг университет или друга учебна институция. Разбирам, че ако се установи плагиатство в работата ми ще получа оценка „Слаб“.

**Подпис:** ____________________  **Дата:** ____________________

---

## Съдържание

1. [Увод](#1-увод)  
2. [Преглед на областта: семантично търсене в електронна търговия](#2-преглед-на-областта-семантично-търсене-в-електронна-търговия)  
3. [Проектиране](#3-проектиране)  
4. [Реализация, тестване/експерименти](#4-реализация-тестванеексперименти)  
5. [Заключение](#5-заключение)  
6. [Използвана литература](#6-използвана-литература)  

---

## 1. Увод

### 1.1. Мотивация

Търсенето в продуктови каталози в електронната търговия често е ограничено до ключови думи и прости филтри. Това води до слаби резултати при:

- свободно формулирани заявки („лека бюджетна кола/велосипед за багаж“);
- „неекспертни“ потребители, които описват нужда, а не точни спецификации;
- синоними и близки значения („cheap“ ≈ „affordable“, „hills“ ≈ „climbing“);
- граматически зависимости („bike with child seat“ vs „bike for child“).

Проектът цели да надгради класическите методи от *Information Retrieval (IR)* с техники от *Natural Language Processing (NLP)*, така че да се постигне по-смислено („семантично“) съвпадение между заявка и продукт.

### 1.2. Цел и задачи

**Цел:** да се реализира домейн-специфична търсачка за продукти, която комбинира класически IR (TF‑IDF/косинусна близост или BM25 в Elasticsearch) с NLP модули за извличане на ограничения и семантично обогатяване, и която предоставя уеб интерфейс за конфигуриране и сравнение на различни режими на търсене.

**Основни задачи:**

- зареждане и нормализиране на продуктов набор от данни (CSV);
- предобработка на текст (нормализация, стоп-думи, стеминг);
- извличане на ограничения от заявка (цена/тегло) и разпознаване на марки;
- семантично обогатяване на документи с тагове (price/weight/category);
- индексиране и търсене в два режима:
  - локално TF‑IDF + косинусна близост;
  - Elasticsearch индекс (BM25 + филтри) и по избор плътни вектори;
- разширяване на заявки чрез:
  - *dependency parsing* (spaCy) за връзки и съставни фрази;
  - *word embeddings* (Sentence Transformers) за синоними/семантични разширения;
- предоставяне на Flask уеб интерфейс за експерименти и сравнение;
- тестване с примерни заявки и автоматизирани проверки.

---

## 2. Преглед на областта: семантично търсене в електронна търговия

### 2.1. Подходи и методи

В практиката се срещат няколко нива на търсене:

- **Ключоводумно търсене** — съвпадения по термини, често с ранжиране от тип TF‑IDF/BM25.
- **Фасетни/параметрични филтри** — филтриране по цена, категория, марка и др. (точни ограничения).
- **Семантично търсене** — използване на векторни представяния (*embeddings*) за близост в смислово пространство.
- **Хибридно търсене** — комбиниране на класическо IR ранжиране с векторна близост и/или NLP разширения.

Проектът реализира хибриден подход: първо се извличат ограничения (филтри), после се прилага ранжиране (TF‑IDF/косинус или BM25), а при включени разширения — зависимостни токени и/или *embeddings* участват в търсенето и скоринга.

### 2.2. Съществуващи решения

- **Elasticsearch** е индустриален стандарт за пълнотекстово търсене (BM25, анализатори, филтри) и предлага механизми за векторно търсене.
- За NLP анализ често се използват **NLTK** (токенизация, стоп-думи, стеминг) и **spaCy** (модели за *dependency parsing*).
- За семантични векторни представяния — **Sentence Transformers** (на база *transformers* модели).

### 2.3. Сравнителен анализ (в рамките на проекта)

В проекта могат да се сравняват конфигурации:

- TF‑IDF (локално) vs Elasticsearch (BM25);
- без разширения vs с *dependency parsing* (структурни токени);
- без разширения vs с *embeddings* (семантична близост);
- хибридно скориране (TF‑IDF + *embeddings*) с параметър `hybrid_alpha`.

Очаквани резултати:

- TF‑IDF е бърз и прост, но страда при синоними и парафрази.
- Elasticsearch подобрява ранжиране и гъвкавост (анализатори/филтри), но без вектори пак е „терминен“ метод.
- *Dependency parsing* помага при фрази и предлози („with/for“), като въвежда токени от тип `bike_with_child_seat`.
- *Embeddings* помагат при семантични синоними („cheap“ → „budget“, „affordable“) и тематични близости („hills“ → „mountain“, „climbing“).

---

## 3. Проектиране

### 3.1. Анализ на изискванията

**Функционални изисквания:**

- импорт на продуктови данни от CSV (`data/amazon-products.csv` и примерен `data/sample_products.csv`);
- търсене по свободен текст и връщане на топ‑K резултати;
- разпознаване на марка в заявката и филтриране по марка;
- извличане на ценови ограничения (напр. „under 60“; „between 50 and 100“) и прилагането им като филтри;
- по избор: извличане на ограничения за тегло (kg);
- семантично обогатяване с тагове на база правила (цена/тегло/категория);
- конфигурируем backend: локален TF‑IDF или Elasticsearch (Docker);
- уеб интерфейс за конфигуриране/инициализация/търсене.

**Нефункционални изисквания:**

- модулност (ясно разделени компоненти);
- възпроизводимост (docker-compose за Elasticsearch; фиксирани зависимости в `requirements.txt`);
- приемлива производителност за ~1000 продукта и интерактивно търсене.

### 3.2. Обща архитектура (слоеве/модули/компоненти)

Системата е модулна и се управлява от `ProductSearchEngine` (`src/search_engine.py`), който оркестрира:

- **NLPParser** (`src/nlp_parser.py`) — извлича марка и ограничения (цена/тегло) и генерира „чиста“ заявка.
- **TextPreprocessor** (`src/preprocessing.py`) — почистване, стоп-думи, стеминг; при включване — *dependency parsing*.
- **SemanticEnricher** (`src/semantic_enrichment.py`) — добавя тагове и подготвя поле `searchable_content`.
- **Indexer**:
  - локален **SearchIndexer** (`src/indexer.py`) — TF‑IDF + косинусна близост;
  - **ElasticsearchIndexer** (`src/elasticsearch_indexer.py`) — индекс/търсене в Elasticsearch.
- **Web UI**: Flask приложение `app.py` + HTML шаблони в `templates/`.

Схематично (логически поток):

```
Заявка → NLPParser (марка/ограничения) → TextPreprocessor (+ разширения)
     → (филтри) → Indexer (TF‑IDF/ES) → Резултати
                  ↑
       Данни → Enricher → searchable_content → build_index()
```

### 3.3. Модел на данните

**Входни данни (CSV):** продуктът се разглежда като документ в корпус. Основни полета (варират по CSV, нормализират се):

- `title` → вътрешно `name`/`product`
- `description` → вътрешно `desc`
- `final_price`/`price` → `price_numeric`
- `brand`, `categories`/`root_bs_category`, `rating`, `reviews_count`, `availability`, `image_url`, `url`, `item_weight` (ако е налично)

**Вътрешни производни полета:**

- `semantic_tags` — правила/динамични тагове (напр. budget/premium; light/heavy);
- `dependency_features` — токени от зависимостни връзки (при включено *dependency parsing*);
- `searchable_content` — конкатенация на текстови полета + тагове (и по избор dependency features).

**Elasticsearch документен модел:** при ES индексиране се записват полета като `title`, `brand`, `description`, `price`, `category`, `semantic_tags`, `searchable_content`, и по избор `content_vector` (плътен вектор за семантично търсене).

### 3.4. Схема за представяне на знанията

Знанията в системата се представят чрез:

- **Семантични тагове** (*rule-based + optional dynamic*): добавяне на „скрити“ думи, които не присъстват в описанията, но са полезни за търсене (напр. „budget“, „lightweight“).
- **Зависимостни токени** (*dependency features*): токени с подчертавки, описващи връзки и съставни изрази (напр. `bike_with_child_seat`, `mountain_bike`).
- **Векторни представяния** (*embeddings*): плътни вектори на текст, използвани за семантична близост и/или разширяване на заявка.
- **Конфигурационни речници**: списък от марки и семантични групи (в `src/config.py`) за устойчиво разпознаване.

### 3.5. Потребителски интерфейс

Реализирано е уеб приложение (Flask) с основни API маршрути:

- `GET /` — интерфейс за търсене (HTML);
- `GET/POST /api/config` — текуща конфигурация (Elasticsearch, dependency parsing, embeddings, `hybrid_alpha`, `top_k`);
- `POST /api/initialize` — зареждане на данни и построяване на индекс;
- `POST /api/search` — търсене по заявка;
- `GET /api/status` — статус (вкл. връзка към Elasticsearch);
- `POST /api/reset` — ресет на engine.

Целта е да се улесни експериментирането и сравнението на различни режими без промяна в кода.

---

## 4. Реализация, тестване/експерименти

### 4.1. Използвани технологии, платформи и библиотеки

- **Език:** Python  
- **Уеб:** Flask + Flask-CORS  
- **IR/ML:** scikit-learn (TF‑IDF, косинусна близост)  
- **NLP:** NLTK (стоп-думи, стеминг), spaCy (*dependency parsing*)  
- **Семантика:** Sentence Transformers (*embeddings*), (по избор) WordNet за синоними  
- **Търсене:** Elasticsearch 8.x (Docker, `docker-compose.yml`)  
- **Контейнеризация:** Docker Compose за ES backend  

### 4.2. Реализация (модули и логика)

**Зареждане и нормализация на данни.** `ProductSearchEngine.load_data()` чете CSV, нормализира колони към вътрешна схема и почиства стойности (напр. цена/тегло).

**Предобработка.** `TextPreprocessor` извършва:

- почистване и нормализация (lowercase, премахване на символи);
- стоп-думи и стеминг (Snowball Stemmer);
- при включено *dependency parsing* — извличане на съставни фрази и предлогови отношения, и генериране на допълнителни токени за заявка и/или документ.

**NLP анализ на заявка.** `NLPParser.parse_query()` извлича:

- марки чрез речниково търсене;
- ценови ограничения чрез регулярни изрази (under/over/between/exact);
- числови ограничения за тегло (kg);
- „clean query“ без ограничения и марки.

**Семантично обогатяване.** `SemanticEnricher` добавя:

- `semantic_tags` на база правила (напр. категории „budget“, „premium“ по ценови прагове; „light“/„heavy“ по тегло);
- (по избор) динамично разширяване на тагове чрез WordNet/*embeddings*;
- `searchable_content` като конкатенация на релевантни полета.

**Индексиране и търсене.**

- Локален режим: `SearchIndexer` строи TF‑IDF матрица и търси чрез косинусна близост, като преди скориране се прилагат филтри по марка/цена/тегло/категория.
- Elasticsearch режим: `ElasticsearchIndexer` създава индекс с анализатор (lowercase + stopwords + stemmer) и индексира документи. Търсенето комбинира match заявки и филтри; при включени вектори може да се използва `dense_vector` поле.

**Уеб интерфейс.** `app.py` управлява конфигурацията в session, инициализацията (load + build_index) и търсенето чрез JSON API, което позволява интерактивни експерименти през браузър.

### 4.3. Тестване/експерименти и анализ на резултатите

**Набор от данни.** В `data/amazon-products.csv` има **1000** продукта и **891** различни марки (по стойност на поле `brand`). Средната оценка (ако е налична) е около **4.40**.

**Тестови сценарии (примерни заявки):**

- „cheap running shoes under 60“ — семантични тагове + ценови филтър.
- „bike with child seat“ — разлика между ключоводумно търсене и dependency токени `bike_with_child_seat`.
- „cheap bike for hills“ — очаквано подобрение при *embeddings* („hills“ → „mountain/climbing/steep“).
- „Nike running shoes“ — разпознаване на марка и филтриране.

**Автоматизирани проверки.**

В `tests/` има тестове/демонстрации за:

- наличност на директории и шаблони за уеб приложението;
- коректна структура на `app.py` маршрути;
- инстанциране на `ProductSearchEngine` и наличност на основни методи;
- демонстрации за семантично разширяване и двупосочни мапинги.

**Наблюдения и компромиси:**

- *Dependency parsing* увеличава времето за индексиране (еднократна цена), но подобрява съвпадение при сложни фрази.
- *Embeddings* подобряват семантичната устойчивост, но изискват модели и повече ресурси (RAM/време).
- Хибридният параметър `hybrid_alpha` позволява компромис между класическо и семантично скориране.

---

## 5. Заключение

Проектът реализира хибридна търсачка за продукти, която комбинира TF‑IDF/Elasticsearch с NLP модули за извличане на ограничения и семантично обогатяване. Добавените разширения чрез *dependency parsing* и *embeddings* адресират два типични проблема: (1) различни формулировки и синоними, и (2) структурни зависимости във фрази.

**Идеи за бъдещо развитие:**

- по-реалистична оценка на качеството (напр. *precision@k*/*nDCG*) чрез ръчно анотирани заявки;
- извличане на повече атрибути от заявки (размер, цвят, материал и т.н.);
- по-добро управление на домейн речници (марки/категории) и автоматично обновяване;
- оптимизация на индексирането при dependency features (батчове/кеширане);
- поддръжка на многоезични заявки (напр. български).

---

## 6. Използвана литература

1. Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. *Introduction to Information Retrieval*. Cambridge University Press, 2008.
2. Elasticsearch B.V. *Elasticsearch Documentation*. (онлайн ресурс).
3. Bird, Steven, Ewan Klein, and Edward Loper. *Natural Language Processing with Python*. O’Reilly Media, 2009. (NLTK)
4. Honnibal, Matthew, et al. *spaCy: Industrial-Strength Natural Language Processing in Python*. (онлайн ресурс).
5. Reimers, Nils, and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” *EMNLP-IJCNLP*, 2019.
6. Pedregosa, Fabian, et al. “Scikit-learn: Machine Learning in Python.” *Journal of Machine Learning Research*, 2011.

